---
title: "Water Quality Data Analysis System"
author: "APEM Ltd"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
 rmdformats::downcute:
    highlight: tango
    number_sections: yes
    theme: united
editor_options: 
  chunk_output_type: console
---

<img src="ApemLogo.JPG" style="position:absolute;top:0px;right:0px;"/>


```{r setup, include=FALSE}

############################################### OPERATING INSTRUCTIONS ###############################################
#
## Introduction
#
# This is the Water Quality Data Analysis System, developed by APEM Ltd. This workflow will build a  report for given site IDs and return 
#
## Running the Toolbox:
#
# * Create a copy of this RStudio project and, if not present, transfer a copy of this script to the new folder. This project should also contain a folder called "Data", which contains input data files. 
# * Before running the tool, configure the Settings in the section below, which allow users to specify:
#   * Site IDs, both EA and APEM (maximum six sites per run). The order in which sites are specified will be reflecetd in the plots/tables.
#   * A start date for the macroinvertebrate samples (if required).
#   * An end date for the macroinvertebrate samples (if required).
#   * A path to any APEM data to be imported into the tool
#   * A guideline standard to be applied to the LIFE metric plots (0.94 or 1.0)
#   * A flag to indicate whether or not to apply trend lines to the metric plots.
#   * A flag to indicate whether or not to apply confidence intervals to trend lines in metric plots (if applied).
#   * A filter to select data only for specific seasons
#   * Options to set default values for the environmental parameters required for RICT
# * Save a copy of the script with your parameters specified.  
# * To run the script, use the Knit button above. This will render the report as an HTML document, saved to the top level project folder (see the files tab to the right). By default, the report will be called MI_analysis_vX.html (the report name matches the name of this script file). 
######################################################################################################################

knitr::opts_chunk$set(echo = FALSE, warning = FALSE, error = FALSE, message = FALSE)
rm(list = ls())

```

# SETTINGS

```{r settings}

# Specify the name of the file containing APEM data to be imported. 
# Data must be saved in the Data folder, but the folder should not be specified here.
# If not importing APEM data (ie, just using EA data), replace the path with NULL (no quotes)
TEMPLATE_PATH <- "WQ template example_data.xlsx"

# Specify the sheets to be imported from the input template. To import all worksheets enter "all"
# Otherwise list individual worksheets in quotes: c("PROBE_DATA", "MANUAL_FIELD", "LAB_DATA").
SHEETS <- c("PROBE_DATA", "MANUAL_FIELD", "LAB_DATA")

# Start date for samples. Leave as NULL to keep all available data. If specifying date use format "YYYY-MM-DD" (including quotes).
START_DATE <- NULL

# End date for samples. Leave as NULL to keep all available data. If specifying date use format "YYYY-MM-DD" (including quotes).
END_DATE <- NULL

# Specify the parameters to be analysed.
PARAMETERS <- "All"

# Specify all water quality site ID, both EA and APEM, for analysis, in quotes.
# Data in tables and plots in the output report will match the order specified below.
SITES <- "All" 

# Specify a multiplier to be used for handing chemistry results outwith the LOD. Must be between 0 and 1.
# The LOD will be multiplied by this value to generate dummy results.
LOD_MULTIPLIER <- 0.5

# Specify a folder to save output files to
OUTPUT_PATH <- "Outputs/"

```

```{r libraries}

## LIBRARIES

# Load required libraries
if(!require(pacman)) install.packages("pacman")
pacman::p_load(remotes, readxl, dplyr, stringr, lubridate, ggplot2, writexl, 
               tidyr, DT, rnrfa, leaflet, downloader, sf, GGally, gridextra,
               rlang, readr, openxlsx) # new for WQ tool

# Conditionally install hetoolkit from github
if ("hetoolkit" %in% installed.packages() == FALSE) {
  remotes::install_github("APEM-LTD/hetoolkit")
}

library(hetoolkit)

# Add tool-specific functions
source("R/wq_functions.R")

```

```{r process_user_entry}

# if (is.null(TEMPLATE_PATH) == TRUE) {
#   import_APEM <- ""
#   TEMPLATE_PATH <- "NA - no APEM data supplied"
# } else {
#   import_APEM <- "R/WQ_import_APEM.Rmd"
# }

# Process user date entry for reporting
if(is.null(START_DATE) == TRUE) {
  SDATE <- "NA - using all data"
} else {
  SDATE <- as.character(START_DATE)
}

if(is.null(END_DATE) == TRUE) {
  EDATE <- "NA - using all data"
} else {
  EDATE <- as.character(END_DATE)
}

```

```{r entry_check, warning=TRUE, error=FALSE}

## Check input file exists
if(is.null(TEMPLATE_PATH == FALSE && exists(TEMPLATE_PATH) == FALSE)){
  stop("Input template file does not exist")
}

## Sheet names exist in template
if(is.null(TEMPLATE_PATH) == FALSE){
  template_sheets <- readxl::excel_sheets(paste0("Data/", TEMPLATE_PATH))
  sheets_exist <- lapply(SHEETS, function(s) s %in% template_sheets)
  if(FALSE %in% sheets_exist == TRUE){
    stop("Specified sheets do not all exist in input template file")
  }
}

## Start date in correct format
if(is.null(START_DATE) == FALSE && IsDate(START_DATE, "%Y-%m-%d") == FALSE) {
  stop("Start date should be in YYYY-MM-DD format")
}

## End date in correct format
if(is.null(END_DATE) == FALSE && IsDate(END_DATE, "%Y-%m-%d") == FALSE) {
  stop("Start date should be in YYYY-MM-DD format")
}

## End date after start date
if((is.null(START_DATE) == FALSE && as.Date(START_DATE) > Sys.Date()) == TRUE) {
  stop("Start data cannot be in the future")
}

## Start date after 2000 (WQA database cut off for EA data)
if((is.null(START_DATE) == FALSE && as.Date(START_DATE) < as.Date("2000-01-01")) == TRUE) {
  print("NOTE: EA data is not availble from the WQA database before year 2000")
}

## LOD multiplier is numeric
if(is.numeric(LOD_MULTIPLIER) == FALSE){
  stop("LOD multiplier must be numeric")
}

## LOD multiplier within valid range
if(LOD_MULTIPLIER < 0 || LOD_MULTIPLIER > 1) {
  stop("LOD multiplier must be between 0 and 1")
}

```

User-specified settings:  

 * Input template path: **`r TEMPLATE_PATH`**  
 * Source sheets to import: **`r SHEETS`**  
 * Start date: **`r SDATE`** 
 * End date: **`r EDATE`** 
 * Parameters of interest: **`r PARAMETERS`**  
 * LOD multiplier: **`r LOD_MULTIPLIER`**
 
```{r import}

# IMPORT FROM TEMPLATE
# Load specified sheets into a list
sheets <- lapply(SHEETS, function(i) readxl::read_excel(paste0("Data/", TEMPLATE_PATH), sheet = i))
names(sheets) <- SHEETS

params <- readxl::read_excel(paste0("Data/", TEMPLATE_PATH),
                             sheet = "PARAMETERS")   # Loads PARAMETERS tab with details of column headers

metadata <- readxl::read_excel(paste0("Data/", TEMPLATE_PATH),
                             sheet = "METADATA") %>%   # Loads PARAMETERS tab with details of column headers)
  na.omit(.data)    # Skip blank rows due to formulae.

standards <- readxl::read_excel(paste0("Data/", TEMPLATE_PATH),
                             sheet = "STANDARDS")   # Loads STANDARDS tab with details of WFD boundaries)

## ERROR CHECK for specified parameters
allowed_params <- params$parameter
if (PARAMETERS != "All"){
  check_names(PARAMETERS, allowed_params)
} else {
  PARAMETERS <- allowed_params
}

```

```{r ea_import}

## AUTO-IMPORT EA DATA
if(is.null(SITES) == FALSE){
  
  if ("All" %in% SITES) {
    SITES <- unique(metadata$EA_ID)
  }
  
  # Pull all data for sites
  ea_wq_data <- hetoolkit::import_wq(sites = SITES,
                                     dets = c(61, 76, 85, 111, 117, 180, 9901)) # fixed for specified WFD parameters
  
  # filter by dates
  if(is.null(START_DATE) == FALSE){
    ea_wq_data <- wq_data %>%
      dplyr::filter(date >= START_DATE)
  }
  
  if(is.null(END_DATE) == FALSE){
    ea_wq_data <- wq_data %>%
      dplyr::filter(date <= END_DATE)
  }
  
  ### Update param names to match template
  ea_wq_data <- ea_wq_data %>%
    dplyr::left_join(params, by = c("det_label" = "WIMS_headers")) %>%
    dplyr::select(c(wq_site_id, date, parameter, det_id, result, unit, qualifier)) %>%
    dplyr::rename(location_name = wq_site_id)
  
  # Restructure
  ea_wq_data <- ea_wq_data %>%
    tidyr::pivot_wider(id_cols = c(location_name, date),
                       names_from = parameter,
                       values_from = result) %>%
    dplyr::mutate(source = "EA_DATA")
  
} else {
  ea_wq_data <- readr::read_csv("Data/wq_data_blank.csv")
}

```

```{r merge}

# Drop empty sheets or add source variable
for (sheet in SHEETS) {
  if (nrow(sheets[[sheet]]) == 0){
    sheets[[sheet]] <- NULL
  } else {
    sheets[[sheet]]$source <- sheet
  }
  
}

## Process probe data
if("PROBE_DATA" %in% names(sheets) == TRUE){
  sheets[["PROBE_DATA"]] <- extract_date_time(sheets[["PROBE_DATA"]], "date_time")
  sheets[["PROBE_DATA"]]$time <- strptime(sheets[["PROBE_DATA"]]$time, format = "%H:%M")
}

merge <- dplyr::bind_rows(sheets) %>%
  dplyr::bind_rows(ea_wq_data)

### Standardise columns: order, adding missing (as NA)
merge <- restructure_wq_data(merge)


### Extract parameter names for later use
param_names <- names(merge)[13:ncol(merge)]

### Pivot long
merge <- merge %>%
  tidyr::pivot_longer(cols = all_of(param_names),
                      names_to = "parameter",
                      values_to = "result")

### Match on metadata
full_data <- merge %>%
  dplyr::left_join(metadata, by = "location_name") %>%
  dplyr::left_join(standards)

### Reorder columns
#full_data <- full_data

```

# Quality Checks

```{r qc_checks}

# Negative results/outside possible range
full_data <- full_data %>%
  dplyr::mutate(exclude = ifelse(result < 0 | result > 100, TRUE, FALSE))   # Add others as required

exclusions_data <- full_data %>%
  dplyr::filter(exclude) %>%
  dplyr::select(location_name, date, source, parameter, result)

if (nrow(exclusions_data) > 0) {
  EXCL_tab <- DT::datatable(exclusions_data)
} else {
  EXCL_tab <- "NA - No excluded results"
}

full_data <- full_data %>%
  dplyr::filter(exclude == FALSE)

```

The following results have been excluded 

**`r EXCL_tab`**

```{r process_outliers}

full_data <- full_data %>%
  dplyr::mutate(outlier = FALSE)

for (l in unique(full_data$location_name)){
  for (p in param_names) {
    temp <- full_data %>%
      dplyr::filter(parameter == p & location_name == l)

    t <- ggplot(temp) +
      aes(x = "", y = result) +
      geom_boxplot() +
      labs(title = p)

    outs <- boxplot.stats(temp$result)$out

    full_data <- full_data %>%
      dplyr::mutate(outlier = ifelse(location_name == l & parameter == p & result %in% outs,
                                     TRUE, outlier))
  }
}

outliers_data <- full_data %>%
  dplyr::filter(outlier) %>%
  dplyr::select(location_name, date, source, parameter, result)

if (nrow(outliers_data) > 0) {
  OUT_tab <- DT::datatable(outliers_data)
} else {
  OUT_tab <- "NA - No potential outliers identified"
}

```

The table below summarises results that have been identified as potential outliers:

**`r OUT_tab`**

```{r process_lod}

### Process LOD results and export/tabulate
full_data <- full_data %>%
  dplyr::mutate(limit = ifelse(grepl("<", result) == TRUE | grepl(">", result) == TRUE,
                               substr(result,regexpr("[[:digit:]]", result), nchar(result)), NA_character_),
                limit = as.numeric(limit),
                original_result = result,
                result = as.numeric(result))

lod_tab_data <- full_data %>%
  dplyr::filter(is.na(limit) == FALSE) %>%
  dplyr::select(location_name, date, source, parameter, limit)

if (nrow(lod_tab_data) > 0) {
  LOD_tab <- DT::datatable(lod_tab_data)
} else {
  LOD_tab <- "NA - No results outwith LOD"
}

```

The table below summarises results that are outwith the limit of detection (LOD):

**`r LOD_tab`**

```{r process_stats}

### Calculate summary statistics from all data and tabulate
summary_1 <- full_data %>%
  dplyr::group_by(location_name, parameter) %>%
  dplyr::summarise(mn = mean(result, na.rm = TRUE),
                   sd = sd(result, na.rm = TRUE),
                   min = min(result, na.rm = TRUE),
                   max = max(result, na.rm = TRUE),
                   percentile = case_when(any(parameter == "temp")    ~ quantile(result, 0.98, na.rm = TRUE),
                                          any(parameter == "RDO_sat") ~ quantile(result, 0.1, na.rm = TRUE),
                                          any(parameter == "pH")      ~ quantile(result, 0.05, na.rm = TRUE),
                                          any(parameter == "BOD")     ~ quantile(result, 0.9, na.rm = TRUE),
                                          any(parameter == "amm_N")   ~ quantile(result, 0.9, na.rm = TRUE),
                                          .default = NA_integer_)
                   ) %>%
  dplyr::ungroup()

### Calculate summary statistics with outliers removed
summary_2 <- full_data %>%
  dplyr::filter(outlier == FALSE) %>%
  dplyr::group_by(location_name, parameter) %>%
  dplyr::summarise(mn = mean(result, na.rm = TRUE),
                   sd = sd(result, na.rm = TRUE),
                   min = min(result, na.rm = TRUE),
                   max = max(result, na.rm = TRUE),
                   percentile = case_when(any(parameter == "temp")    ~ quantile(result, 0.98, na.rm = TRUE),
                                          any(parameter == "RDO_sat") ~ quantile(result, 0.1, na.rm = TRUE),
                                          any(parameter == "pH")      ~ quantile(result, 0.05, na.rm = TRUE),
                                          any(parameter == "BOD")     ~ quantile(result, 0.9, na.rm = TRUE),
                                          any(parameter == "amm_N")   ~ quantile(result, 0.9, na.rm = TRUE),
                                          .default = NA_integer_)
                   ) %>%
  dplyr::ungroup()

```

